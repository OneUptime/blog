# How to Mount an In-Memory Volume on Cloud Run for Sharing Files Between Sidecar Containers

Author: [nawazdhandala](https://www.github.com/nawazdhandala)

Tags: GCP, Cloud Run, Sidecar Containers, In-Memory Volume, Google Cloud

Description: Learn how to mount an in-memory volume on Google Cloud Run to share files between sidecar containers for multi-container service architectures.

---

Cloud Run's multi-container support opened up a lot of possibilities for running sidecar patterns - things like log collectors, proxy agents, or file processors running alongside your main application container. But there is a practical question that comes up quickly: how do these containers actually share data with each other?

The answer is in-memory volumes. Cloud Run lets you mount a tmpfs-backed volume that multiple containers in the same service can access simultaneously. This guide walks through exactly how to set it up, with real examples you can deploy today.

## Why In-Memory Volumes Matter for Sidecars

When you run multiple containers in a single Cloud Run service, each container has its own filesystem by default. There is no shared disk. If your main app writes a file that a sidecar needs to read - say, a generated report that a processing sidecar picks up - you need a shared volume.

In-memory volumes (backed by tmpfs) solve this cleanly. They live in RAM, which means they are fast, but they are also ephemeral. When the instance shuts down, the data is gone. This makes them perfect for temporary file sharing, inter-process communication, and scratch space.

Some common use cases include:

- A web app that generates temporary files for a log-shipping sidecar to pick up
- An nginx proxy sidecar that needs to read configuration generated by the main container
- A data pipeline where one container downloads data and another processes it
- Shared Unix sockets between containers

## Prerequisites

Before you start, make sure you have:

- A GCP project with billing enabled
- The `gcloud` CLI installed and authenticated
- Cloud Run API enabled in your project
- Docker installed locally for building container images

Enable the Cloud Run API if you have not already:

```bash
# Enable the Cloud Run API in your GCP project
gcloud services enable run.googleapis.com
```

## Understanding the Volume Configuration

Cloud Run in-memory volumes use the `emptyDir` volume type with `memory` as the medium. You define the volume at the service level and then mount it into each container that needs access. The key YAML fields look like this:

```yaml
# Volume definition - goes under the service template spec
volumes:
  - name: shared-data        # Name referenced by volume mounts
    emptyDir:
      medium: Memory          # Use RAM-backed tmpfs
      sizeLimit: 128Mi        # Maximum size of the volume
```

The `sizeLimit` is important because this volume consumes your container's memory allocation. If you set a 128Mi volume limit and your container has 512Mi of memory, the volume will eat into that total.

## Step 1: Prepare Your Container Images

Let us build a simple example with two containers. The main app writes files to a shared directory, and a sidecar reads and processes them.

Here is the main application container. It is a simple Python script that writes a timestamp file every few seconds:

```python
# app.py - Main application that writes files to shared volume
import time
import os
from datetime import datetime

SHARED_DIR = "/shared-data"

def main():
    """Write timestamp files to the shared directory for the sidecar to pick up."""
    os.makedirs(SHARED_DIR, exist_ok=True)
    counter = 0

    while True:
        filename = f"{SHARED_DIR}/event_{counter}.txt"
        timestamp = datetime.utcnow().isoformat()

        with open(filename, "w") as f:
            f.write(f"Event {counter} at {timestamp}\n")

        print(f"Wrote {filename}")
        counter += 1
        time.sleep(5)

if __name__ == "__main__":
    main()
```

And here is the sidecar that watches the directory and processes new files:

```python
# sidecar.py - Sidecar that reads and processes files from shared volume
import os
import time

SHARED_DIR = "/shared-data"
processed = set()

def main():
    """Watch the shared directory and process new files as they appear."""
    os.makedirs(SHARED_DIR, exist_ok=True)

    while True:
        for filename in os.listdir(SHARED_DIR):
            filepath = os.path.join(SHARED_DIR, filename)
            if filepath not in processed:
                with open(filepath, "r") as f:
                    content = f.read()
                print(f"Processed: {filename} -> {content.strip()}")
                processed.add(filepath)
                # Clean up after processing
                os.remove(filepath)
        time.sleep(2)

if __name__ == "__main__":
    main()
```

Build and push both images to Artifact Registry:

```bash
# Create the Artifact Registry repository if it does not exist
gcloud artifacts repositories create my-repo \
  --repository-format=docker \
  --location=us-central1

# Build and push the main app
docker build -t us-central1-docker.pkg.dev/MY_PROJECT/my-repo/main-app:latest -f Dockerfile.main .
docker push us-central1-docker.pkg.dev/MY_PROJECT/my-repo/main-app:latest

# Build and push the sidecar
docker build -t us-central1-docker.pkg.dev/MY_PROJECT/my-repo/sidecar:latest -f Dockerfile.sidecar .
docker push us-central1-docker.pkg.dev/MY_PROJECT/my-repo/sidecar:latest
```

## Step 2: Define the Cloud Run Service YAML

This is where the volume and multi-container setup comes together. Create a `service.yaml` file:

```yaml
# service.yaml - Cloud Run service with shared in-memory volume
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: shared-volume-demo
  annotations:
    run.googleapis.com/launch-stage: BETA
spec:
  template:
    metadata:
      annotations:
        # Enable multi-container support
        run.googleapis.com/container-dependencies: '{"sidecar":["main-app"]}'
    spec:
      containers:
        # Main application container
        - name: main-app
          image: us-central1-docker.pkg.dev/MY_PROJECT/my-repo/main-app:latest
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: shared-data
              mountPath: /shared-data    # Path inside the container
          resources:
            limits:
              memory: 512Mi
              cpu: "1"

        # Sidecar container
        - name: sidecar
          image: us-central1-docker.pkg.dev/MY_PROJECT/my-repo/sidecar:latest
          volumeMounts:
            - name: shared-data
              mountPath: /shared-data    # Same path, same volume
          resources:
            limits:
              memory: 256Mi
              cpu: "0.5"

      # Define the shared in-memory volume
      volumes:
        - name: shared-data
          emptyDir:
            medium: Memory
            sizeLimit: 64Mi
```

Notice a few things here. The `container-dependencies` annotation tells Cloud Run to start the main app before the sidecar. Both containers mount the `shared-data` volume at `/shared-data`. The volume is defined once and referenced by name.

## Step 3: Deploy the Service

Deploy using the YAML file:

```bash
# Deploy the multi-container service with shared volume
gcloud run services replace service.yaml --region=us-central1
```

You can also do this entirely from the command line using `gcloud run deploy` with the `--add-volume` and `--add-volume-mount` flags, though the YAML approach is cleaner for multi-container setups:

```bash
# Alternative: deploy with gcloud flags (single command)
gcloud run deploy shared-volume-demo \
  --region=us-central1 \
  --image=us-central1-docker.pkg.dev/MY_PROJECT/my-repo/main-app:latest \
  --add-volume=name=shared-data,type=in-memory,size-limit=64Mi \
  --add-volume-mount=volume=shared-data,mount-path=/shared-data
```

## Step 4: Verify the Volume Is Working

Check the logs to confirm both containers are communicating through the shared volume:

```bash
# View interleaved logs from both containers
gcloud logging read "resource.type=cloud_run_revision \
  AND resource.labels.service_name=shared-volume-demo" \
  --limit=20 \
  --format="table(timestamp, jsonPayload.message)"
```

You should see the main app writing files and the sidecar processing them in sequence.

## Memory Considerations

Since in-memory volumes use RAM, you need to account for this in your memory limits. Here is a rough guideline:

- Add the volume `sizeLimit` to your total memory budget
- Each container's memory limit is separate from the volume
- If the volume fills up, writes will fail with ENOSPC errors
- Monitor memory usage through Cloud Monitoring

A practical formula: set your container memory to what the app needs, plus a buffer, and keep the volume size as small as possible for your workload.

## Common Pitfalls

There are a few things that trip people up with this setup.

First, the volume is scoped to a single instance. If Cloud Run scales to multiple instances, each instance gets its own independent volume. There is no sharing across instances.

Second, do not use in-memory volumes for data you cannot afford to lose. Instance shutdown means the data disappears. If you need persistence, use Cloud Storage or a database.

Third, watch your total memory consumption. The volume size counts against the instance memory. If your containers plus volume exceed available memory, the instance will be terminated.

## Wrapping Up

In-memory volumes on Cloud Run give you a simple, fast way to share data between sidecar containers. The setup is straightforward - define an emptyDir volume with Memory medium, mount it in each container, and you are done. Just keep the ephemeral nature in mind and size your memory limits accordingly.

For production workloads, consider adding health checks, proper error handling for filesystem operations, and monitoring on the volume usage. The sidecar pattern combined with shared volumes opens up clean architectures for logging, proxying, and data processing on Cloud Run.
