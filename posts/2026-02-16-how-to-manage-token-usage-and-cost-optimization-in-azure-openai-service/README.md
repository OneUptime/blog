# How to Manage Token Usage and Cost Optimization in Azure OpenAI Service

Author: [nawazdhandala](https://www.github.com/nawazdhandala)

Tags: Azure, OpenAI, Cost Optimization, Tokens, GPT-4, Monitoring, FinOps

Description: Practical strategies for monitoring and reducing token usage in Azure OpenAI Service to keep costs under control without sacrificing quality.

---

Azure OpenAI Service bills you based on the number of tokens processed. Every prompt you send and every response you receive consumes tokens, and the costs can add up quickly - especially with GPT-4. A single poorly designed prompt or a runaway batch job can burn through thousands of dollars in hours. In this post, I will walk through practical strategies for monitoring token usage, understanding the pricing model, and optimizing costs without sacrificing the quality of your AI outputs.

## Understanding Token Pricing

Tokens are the fundamental billing unit in Azure OpenAI. A token is roughly 4 characters or about 0.75 words in English. Every API call has two token components:

- **Prompt tokens (input)**: The tokens in your system message, conversation history, and user message.
- **Completion tokens (output)**: The tokens generated by the model in its response.

Both input and output tokens are billed, but at different rates. For GPT-4, the pricing at the time of writing is approximately:

| Model | Input (per 1K tokens) | Output (per 1K tokens) |
|-------|----------------------|----------------------|
| GPT-4 (8K) | $0.03 | $0.06 |
| GPT-4-32K | $0.06 | $0.12 |
| GPT-4-Turbo | $0.01 | $0.03 |
| GPT-3.5-Turbo | $0.0005 | $0.0015 |

Notice that GPT-4 output tokens cost twice as much as input tokens. Also notice that GPT-3.5-Turbo is roughly 60x cheaper than GPT-4. These cost differences are the foundation of most optimization strategies.

## Step 1: Monitor Your Token Usage

You cannot optimize what you do not measure. Start by setting up monitoring for your Azure OpenAI resource.

### Enable Diagnostic Logging

In the Azure Portal, navigate to your Azure OpenAI resource. Click "Diagnostic settings" and create a new setting that sends logs to a Log Analytics workspace.

Once enabled, you can query token usage with KQL (Kusto Query Language):

```kql
// Query token usage by deployment over the last 7 days
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where Category == "RequestResponse"
| where TimeGenerated > ago(7d)
| extend promptTokens = toint(properties_s.promptTokens)
| extend completionTokens = toint(properties_s.completionTokens)
| summarize
    TotalPromptTokens = sum(promptTokens),
    TotalCompletionTokens = sum(completionTokens),
    TotalRequests = count()
    by bin(TimeGenerated, 1h)
| order by TimeGenerated desc
```

### Track Usage in Application Code

You can also track token usage directly from API responses. Every response includes a `usage` object with exact token counts.

```python
response = client.chat.completions.create(
    model="gpt4-production",
    messages=messages,
    max_tokens=500
)

# Extract token usage from the response
usage = response.usage
print(f"Prompt tokens: {usage.prompt_tokens}")
print(f"Completion tokens: {usage.completion_tokens}")
print(f"Total tokens: {usage.total_tokens}")

# Calculate cost (example rates for GPT-4)
input_cost = (usage.prompt_tokens / 1000) * 0.03
output_cost = (usage.completion_tokens / 1000) * 0.06
total_cost = input_cost + output_cost
print(f"Estimated cost: ${total_cost:.4f}")
```

Log these values to your application monitoring system. Over time, you will be able to identify which features, users, or workflows consume the most tokens.

## Step 2: Optimize Prompt Length

Prompt tokens often represent the largest portion of your costs, especially in conversational applications where the entire message history is sent with each request.

### Trim Conversation History

In a chatbot scenario, do not send the entire conversation history with every request. Keep only the most recent N messages or summarize older messages.

```python
def trim_conversation(messages, max_messages=10):
    """
    Keep only the system message and the most recent messages.
    This prevents token usage from growing linearly with conversation length.
    """
    system_messages = [m for m in messages if m["role"] == "system"]
    non_system = [m for m in messages if m["role"] != "system"]

    # Keep only the last max_messages turns
    trimmed = non_system[-max_messages:]

    return system_messages + trimmed
```

### Shorten System Prompts

System prompts are sent with every single request. A 500-token system prompt in a chatbot with 10,000 daily requests costs you 5 million prompt tokens per day just for the system message. Review your system prompts and remove any unnecessary instructions, examples, or formatting guidelines.

### Use Concise Formatting

When passing context (like retrieved documents in a RAG system), strip out unnecessary whitespace, headers, and boilerplate. Every token counts.

## Step 3: Control Output Length

Output tokens cost more than input tokens for GPT-4. Setting a `max_tokens` limit prevents the model from generating unnecessarily long responses.

```python
# Set a reasonable max_tokens limit based on your use case
response = client.chat.completions.create(
    model="gpt4-production",
    messages=messages,
    max_tokens=300,  # Limit output to roughly 225 words
    temperature=0.5
)
```

You can also instruct the model in the system prompt to be concise: "Respond in 2-3 sentences" or "Keep your answer under 100 words."

## Step 4: Route to Cheaper Models

Not every request needs GPT-4. Many tasks - simple Q&A, text classification, formatting, data extraction - work perfectly well with GPT-3.5-Turbo at a fraction of the cost.

Build a routing layer that sends requests to different models based on complexity:

```python
def route_request(query, complexity="auto"):
    """
    Route requests to the appropriate model based on task complexity.
    Uses GPT-3.5-Turbo for simple tasks and GPT-4 for complex ones.
    """
    if complexity == "auto":
        # Simple heuristic: short queries and classification tasks use the cheaper model
        simple_keywords = ["classify", "extract", "format", "summarize briefly"]
        if any(kw in query.lower() for kw in simple_keywords):
            complexity = "simple"
        else:
            complexity = "complex"

    model = "gpt35-turbo-deployment" if complexity == "simple" else "gpt4-production"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": query}],
        max_tokens=500
    )
    return response
```

A more sophisticated approach would use a lightweight classifier to determine which model to use, or use GPT-3.5-Turbo as a first pass and only escalate to GPT-4 when confidence is low.

## Step 5: Implement Caching

Many applications send identical or near-identical prompts repeatedly. A caching layer can eliminate redundant API calls entirely.

```python
import hashlib
import json

# Simple in-memory cache (use Redis or similar for production)
response_cache = {}

def cached_completion(messages, model="gpt4-production", max_tokens=500):
    """
    Cache API responses based on the input messages.
    Identical prompts return cached results instead of making new API calls.
    """
    # Create a cache key from the messages
    cache_key = hashlib.sha256(
        json.dumps(messages, sort_keys=True).encode()
    ).hexdigest()

    if cache_key in response_cache:
        print("Cache hit - returning cached response")
        return response_cache[cache_key]

    # Cache miss - make the API call
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens
    )

    # Store in cache
    response_cache[cache_key] = response.choices[0].message.content
    return response_cache[cache_key]
```

For production systems, use a distributed cache like Redis with a TTL (time to live) so cached responses expire after a reasonable period.

## Step 6: Use Batching for Bulk Operations

If you are processing large volumes of data (like summarizing thousands of documents), use the Azure OpenAI batch API instead of making individual real-time requests. The batch API offers a 50% discount on token costs compared to real-time API calls.

## Step 7: Set Up Budget Alerts

Azure Cost Management allows you to set budget alerts so you are notified before costs spiral out of control.

In the Azure Portal, navigate to "Cost Management + Billing." Create a budget for your Azure OpenAI resource group and set alert thresholds at 50%, 75%, and 90% of your budget. You can receive alerts via email or trigger Azure Functions for automated responses (like temporarily disabling a deployment).

## Step 8: Configure Rate Limits

Each model deployment in Azure OpenAI has a configurable tokens-per-minute (TPM) rate limit. Setting this appropriately serves two purposes: it prevents runaway costs and it ensures fair usage across your applications.

In Azure OpenAI Studio, go to Deployments, select your deployment, and adjust the TPM rate limit. Start conservative and increase as needed.

## Cost Comparison Example

Let us compare costs for a typical chatbot scenario processing 50,000 requests per day with an average of 800 prompt tokens and 200 completion tokens per request.

| Model | Daily Input Cost | Daily Output Cost | Daily Total | Monthly Total |
|-------|-----------------|-------------------|-------------|---------------|
| GPT-4 | $1,200 | $600 | $1,800 | $54,000 |
| GPT-4-Turbo | $400 | $300 | $700 | $21,000 |
| GPT-3.5-Turbo | $20 | $15 | $35 | $1,050 |

The difference is staggering. By routing 80% of simple queries to GPT-3.5-Turbo and only using GPT-4 for complex ones, you could bring that monthly cost down from $54,000 to under $12,000 while still delivering high-quality answers where it matters.

## Wrapping Up

Token cost management is not an afterthought - it needs to be part of your architecture from day one. Start by setting up monitoring so you can see where tokens are being spent. Then apply the optimization strategies that make the most sense for your workload: prompt trimming, output limits, model routing, and caching. The combination of these techniques can easily reduce costs by 50-80% without meaningfully impacting the quality of your AI outputs.
