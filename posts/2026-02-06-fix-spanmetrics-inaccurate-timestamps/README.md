# How to Fix the spanmetrics Processor Producing Inaccurate Timestamps on Derived Metrics

Author: [nawazdhandala](https://www.github.com/nawazdhandala)

Tags: OpenTelemetry, spanmetrics, Processor, Metrics

Description: Fix inaccurate timestamps on metrics generated by the spanmetrics processor caused by aggregation timing misalignment.

The `spanmetrics` processor (or connector, in newer Collector versions) generates metrics from spans, giving you RED metrics (Rate, Errors, Duration) without additional instrumentation. But you notice the timestamps on these derived metrics do not align with the actual span timestamps. Metrics might show a spike 30 seconds after the actual spike in traffic, or metric start times are wrong, causing backends to reject or misinterpret the data.

## Understanding the Problem

The spanmetrics processor aggregates spans into metrics over time intervals. The timestamp on the resulting metric is determined by the aggregation interval, not by the individual span timestamps. This creates a few issues:

1. **Metric timestamps lag behind span timestamps** by up to one aggregation interval
2. **Start times can be incorrect** when the processor first starts, causing cumulative metrics to show wrong rates
3. **Backends reject metrics** when timestamps appear to go backwards or have gaps

## Diagnosing the Issue

Check the Collector logs for timestamp-related warnings:

```bash
kubectl logs -n observability deployment/otel-collector | grep -i "timestamp\|time\|spanmetrics"

# Common messages:
# "dropping data point with start time before previous data point"
# "metric timestamp is before the start timestamp"
```

Compare span timestamps with metric timestamps in your backend:

```promql
# Check when the metric was actually recorded vs when spans arrived
# The difference shows the lag introduced by the processor
```

## The spanmetrics Configuration

Here is a typical spanmetrics setup that may produce timestamp issues:

```yaml
# Older configuration (processor - deprecated)
processors:
  spanmetrics:
    metrics_exporter: prometheus
    dimensions:
      - name: http.method
      - name: http.status_code
    histogram:
      explicit:
        buckets: [5ms, 10ms, 25ms, 50ms, 100ms, 250ms, 500ms, 1s, 5s, 10s]

# Newer configuration (connector)
connectors:
  spanmetrics:
    histogram:
      explicit:
        buckets: [5ms, 10ms, 25ms, 50ms, 100ms, 250ms, 500ms, 1s, 5s, 10s]
    dimensions:
      - name: http.method
      - name: http.status_code
    metrics_flush_interval: 15s  # This affects timestamp accuracy
```

## Fix 1: Reduce the Flush Interval

A shorter flush interval means metrics are exported more frequently, reducing the timestamp lag:

```yaml
connectors:
  spanmetrics:
    metrics_flush_interval: 5s  # Reduce from default 15s to 5s
    histogram:
      explicit:
        buckets: [5ms, 10ms, 25ms, 50ms, 100ms, 250ms, 500ms, 1s, 5s, 10s]
```

The trade-off is more frequent exports and slightly higher resource usage. But the metric timestamps will be much closer to the actual span timestamps.

## Fix 2: Use Delta Temporality

Cumulative temporality metrics track values from a fixed start time, which makes timestamp issues more visible. Switching to delta temporality can reduce problems:

```yaml
connectors:
  spanmetrics:
    metrics_flush_interval: 15s
    aggregation_temporality: "AGGREGATION_TEMPORALITY_DELTA"
    histogram:
      explicit:
        buckets: [5ms, 10ms, 25ms, 50ms, 100ms, 250ms, 500ms, 1s, 5s, 10s]
```

Note that your backend must support delta temporality. Prometheus expects cumulative, while OTLP backends typically support both.

## Fix 3: Align with the Batch Processor

If the batch processor flushes spans at a different interval than the spanmetrics processor flushes metrics, you get misalignment. Align them:

```yaml
processors:
  batch:
    send_batch_size: 512
    timeout: 5s  # Match this with the spanmetrics flush interval

connectors:
  spanmetrics:
    metrics_flush_interval: 5s  # Same as batch timeout
```

## Fix 4: Handle Start Time Correctly

When the Collector restarts, the spanmetrics processor loses its state. The first metric export after a restart might have a start time of "now," which creates a discontinuity in cumulative metrics:

```yaml
connectors:
  spanmetrics:
    # Use resource_metrics_key_attributes to maintain consistent metric identity
    resource_metrics_key_attributes:
      - service.name
      - service.namespace
    metrics_flush_interval: 15s
```

Some backends handle this by detecting resets in cumulative counters. Make sure your backend is configured for this:

```yaml
# For Prometheus remote-write backends
exporters:
  prometheusremotewrite:
    endpoint: "https://prometheus.example.com/api/v1/write"
    # The remote-write protocol handles counter resets natively
```

## Fix 5: Use the Correct Pipeline Order

Make sure spans flow through the spanmetrics connector before being exported:

```yaml
service:
  pipelines:
    # Spans come in here
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [spanmetrics, otlp/traces]  # spanmetrics as connector

    # Metrics generated by spanmetrics go here
    metrics/spanmetrics:
      receivers: [spanmetrics]
      processors: [batch]
      exporters: [otlp/metrics]
```

A common mistake is putting the spanmetrics connector after the batch processor in a way that creates buffering delays.

## Fix 6: Use Exponential Histograms

Exponential histograms are more efficient and produce fewer timestamp-related issues because they adapt their buckets automatically:

```yaml
connectors:
  spanmetrics:
    histogram:
      exponential:
        max_size: 160  # Number of buckets
    metrics_flush_interval: 15s
```

## Verification

After making changes, verify the metrics look correct:

```bash
# Export metrics to the debug exporter to inspect timestamps
exporters:
  debug:
    verbosity: detailed

# Check that metric timestamps are within the expected range
# Metric timestamp should be close to the current time
# Start timestamp should be the time the Collector started (for cumulative)
# or the start of the current interval (for delta)
```

The spanmetrics processor is a powerful tool for generating RED metrics from traces, but it introduces an inherent lag due to aggregation. Reduce the flush interval and align it with your batch processor to minimize timestamp inaccuracy. For backends that are sensitive to timestamp ordering, delta temporality is often the safer choice.
