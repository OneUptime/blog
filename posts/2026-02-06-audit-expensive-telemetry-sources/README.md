# How to Audit and Identify the Top 10 Most Expensive Telemetry Sources in Your OpenTelemetry Pipeline

Author: [nawazdhandala](https://www.github.com/nawazdhandala)

Tags: OpenTelemetry, Cost Optimization, Telemetry Audit, Observability

Description: A practical guide to finding and ranking the most expensive telemetry sources in your OpenTelemetry pipeline using Collector metrics and analysis queries.

Most engineering teams have no idea which services, teams, or signal types are responsible for the bulk of their observability costs. Telemetry volume tends to follow a power law - a handful of sources generate the majority of data. Finding those sources is the first step to controlling costs.

This post shows you how to instrument your OpenTelemetry Collector to track telemetry volume by source, then query that data to rank the top offenders.

## The Problem: Invisible Costs

When every microservice sends traces, metrics, and logs through the same Collector pipeline, it is easy to lose track of who is sending what. A single chatty service producing debug-level logs can account for 40% of your total telemetry volume. Without visibility into per-source volume, cost optimization is guesswork.

## Step 1: Enable Collector Internal Telemetry

The OpenTelemetry Collector exposes its own metrics about data flowing through the pipeline. These internal metrics are the foundation for your audit.

Configure the Collector to expose Prometheus metrics about itself:

```yaml
# otel-collector-config.yaml
# Enable internal telemetry so you can measure data volume per pipeline component

service:
  telemetry:
    metrics:
      level: detailed
      address: 0.0.0.0:8888
```

The `detailed` level exposes metrics like `otelcol_receiver_accepted_spans`, `otelcol_receiver_accepted_metric_points`, and `otelcol_receiver_accepted_log_records`, broken down by receiver and transport.

## Step 2: Add Resource Attributes for Cost Attribution

To attribute telemetry volume to specific services, make sure every application sets the `service.name` and `service.namespace` resource attributes. The OpenTelemetry SDK does this by default, but you should also add a cost-attribution attribute like `team.name` or `department`.

Here is an example using the OpenTelemetry Python SDK:

```python
# Configure the SDK with cost-attribution resource attributes
# These attributes will be attached to every span, metric, and log record

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.resources import Resource

resource = Resource.create({
    "service.name": "payment-service",
    "service.namespace": "checkout",
    "team.name": "payments-team",       # Custom attribute for cost tracking
    "environment": "production",
})

provider = TracerProvider(resource=resource)
trace.set_tracer_provider(provider)
```

## Step 3: Use the Count Connector to Track Volume

The OpenTelemetry Collector has a `count` connector that counts telemetry items passing through the pipeline. This connector generates metrics from spans, logs, and metrics, which you can then export to a metrics backend.

```yaml
# otel-collector-config.yaml
# Use the count connector to generate volume metrics per service

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

connectors:
  # The count connector turns telemetry items into count metrics
  # grouped by resource attributes
  count:
    spans:
      otel.span.count:
        description: "Count of spans by service"
        conditions:
          - "true"
        attributes:
          - key: service.name
          - key: team.name
    logs:
      otel.log.count:
        description: "Count of log records by service"
        conditions:
          - "true"
        attributes:
          - key: service.name
          - key: team.name
    metrics:
      otel.metric.count:
        description: "Count of metric data points by service"
        conditions:
          - "true"
        attributes:
          - key: service.name
          - key: team.name

exporters:
  prometheus:
    endpoint: 0.0.0.0:9090

  otlp:
    endpoint: "backend:4317"
    tls:
      insecure: true

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [count, otlp]
    logs:
      receivers: [otlp]
      exporters: [count, otlp]
    metrics/count:
      # This pipeline receives the count metrics generated by the connector
      receivers: [count]
      exporters: [prometheus]
    metrics:
      receivers: [otlp]
      exporters: [count, otlp]
```

## Step 4: Query for the Top 10 Most Expensive Sources

Once the count metrics are flowing into Prometheus, you can query for the heaviest hitters.

This PromQL query ranks services by total span volume over the last 24 hours:

```promql
# Top 10 services by span count in the last 24 hours
topk(10,
  sum by (service_name, team_name) (
    increase(otel_span_count_total[24h])
  )
)
```

For log volume:

```promql
# Top 10 services by log record count in the last 24 hours
topk(10,
  sum by (service_name, team_name) (
    increase(otel_log_count_total[24h])
  )
)
```

## Step 5: Estimate Dollar Costs

If you know your vendor's pricing per unit, you can estimate costs directly. Most vendors charge per GB ingested. You need to multiply the count by the average size per telemetry item.

Here is a Python script that pulls from Prometheus and estimates costs:

```python
# estimate_costs.py
# Pull volume metrics from Prometheus and estimate monthly costs per service

import requests

PROMETHEUS_URL = "http://localhost:9090"
COST_PER_SPAN = 0.000003      # $3 per million spans (typical SaaS pricing)
COST_PER_LOG = 0.000002       # $2 per million log records
COST_PER_METRIC = 0.000001    # $1 per million metric data points

def query_prometheus(promql):
    """Run a PromQL instant query and return the result vector."""
    resp = requests.get(f"{PROMETHEUS_URL}/api/v1/query", params={"query": promql})
    return resp.json()["data"]["result"]

# Get 30-day span volume by service
span_results = query_prometheus(
    'sum by (service_name, team_name) (increase(otel_span_count_total[30d]))'
)

print("Service | Team | Monthly Spans | Estimated Cost")
print("-" * 60)

for result in sorted(span_results, key=lambda r: float(r["value"][1]), reverse=True)[:10]:
    service = result["metric"].get("service_name", "unknown")
    team = result["metric"].get("team_name", "unknown")
    count = float(result["value"][1])
    cost = count * COST_PER_SPAN
    print(f"{service} | {team} | {count:,.0f} | ${cost:,.2f}")
```

## Step 6: Take Action on Findings

Once you have your ranked list, the typical actions are:

1. **Drop debug-level logs** from the top offenders using the `filter` processor
2. **Sample traces** from high-volume, low-value services using the `tail_sampling` processor
3. **Reduce metric cardinality** by dropping unused labels with the `metricstransform` processor
4. **Set per-service rate limits** using the `rate_limiter` extension

Here is a quick filter processor config to drop debug logs from a noisy service:

```yaml
processors:
  filter/drop-debug:
    logs:
      # Drop debug and trace level logs from the payment-service
      # which was identified as the top log producer
      log_record:
        - 'severity_number < 9 and resource.attributes["service.name"] == "payment-service"'
```

## Making This Repeatable

Run this audit monthly. Telemetry patterns change as teams ship new features. A service that was quiet last month might become your most expensive source after a new deployment adds verbose logging.

Build a dashboard that shows volume trends per service over time. When a service suddenly spikes, you can catch it before it hits your next invoice. The combination of the count connector and a Prometheus backend gives you everything you need to keep telemetry costs visible and under control.
