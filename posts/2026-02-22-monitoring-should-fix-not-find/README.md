# Why Your Monitoring Tool Should Fix Problems, Not Just Find Them

Author: [mallersjamie](https://www.github.com/mallersjamie)

Tags: Observability, Monitoring, DevOps, Open Source

Description: The observability industry optimized for finding problems. The next leap is fixing them automatically.

Here's a mass delusion the infrastructure industry has been operating under for about a decade: if we can just *detect* problems faster, everything will be fine.

We built better dashboards. Better alerts. Better correlations. Better traces. We went from Nagios to Datadog, from static thresholds to anomaly detection, from single metrics to distributed tracing across hundreds of microservices. And after all of that investment — billions of dollars across the industry — the median time to *resolve* an incident has barely moved.

Why? Because we optimized the wrong bottleneck.

## The Alerting Treadmill

Let's start with some uncomfortable numbers. Studies consistently show that over 90% of alerts generated by monitoring tools are noise — false positives, known issues, transient blips, or duplicates of the same underlying problem. The average on-call engineer at a mid-size company receives hundreds of alerts per week. Most of them mean nothing. The ones that do mean something get buried in the ones that don't.

This isn't a configuration problem. You can spend weeks tuning thresholds, building runbooks, and setting up escalation policies. You'll reduce noise by maybe 30-40%. Then your system changes — a new deployment, a traffic pattern shift, a dependency update — and you're back to square one.

The industry's answer to this has been... more alerting. Smarter alerting. AI-powered alerting. "We'll use machine learning to reduce alert fatigue!" Great. Now instead of 500 meaningless alerts, you get 300 meaningless alerts and a black-box model you can't debug when it misses something critical.

Alert fatigue isn't a bug in the system. It's a fundamental consequence of the system's design. When your tool's job ends at "something might be wrong," you've built an anxiety machine, not a solution.

## The Bottleneck Moved

Think about the incident lifecycle: detect → triage → diagnose → fix → verify → close.

The entire observability industry — Datadog, New Relic, Grafana, PagerDuty, Splunk, the whole lot — has been laser-focused on the first three steps. Detect faster. Triage smarter. Diagnose with better correlation. And they've gotten genuinely good at it. A modern observability stack can tell you that your p99 latency spiked because a specific pod in a specific service started throwing OOM errors after a deployment at 3:47 AM, correlated with a config change in an upstream dependency.

Impressive. Truly. But now what?

Now a human wakes up. Reads the alert. Opens six tabs. Reads logs. Cross-references with the deployment timeline. Figures out the fix. Tests it. Deploys it. Verifies. Goes back to bed at 5 AM and shows up to standup looking like death.

The bottleneck isn't detection anymore. It hasn't been for years. The bottleneck is *remediation* — the actual act of fixing the problem. And almost nobody in the industry is working on that.

## Why Remediation Was "Impossible" (Until Now)

There's a reason the industry stopped at detection. Fixing things automatically is genuinely hard. A monitoring tool can observe your system from the outside. But fixing it requires *understanding* — understanding the code, the architecture, the intent behind a configuration, the blast radius of a change.

For decades, the best we could do was crude automation: restart the pod, scale up the replicas, roll back the deployment. These are blunt instruments. They work for a narrow class of problems and create new problems for everything else. Auto-scaling when the real issue is a memory leak just makes your cloud bill explode. Auto-rollback when the "bad" deployment actually fixed a different critical bug puts you in a worse state than before.

Real remediation requires reasoning. You need to read the error, understand the context, look at the code, figure out what changed, and determine the right fix. That was firmly in the domain of human cognition.

Until large language models showed up and got unreasonably good at exactly this kind of reasoning.

## AI Changes Everything (No, Really This Time)

I know. Everyone's tired of "AI changes everything" takes. But hear me out, because this isn't about chatbots or generating marketing copy. This is about a specific, concrete capability that didn't exist two years ago: LLMs can read a stack trace, correlate it with recent code changes, understand the semantic meaning of what went wrong, and suggest a fix that's actually correct a meaningful percentage of the time.

Not 100% of the time. Not even 80%. But enough that the workflow changes fundamentally. Instead of "wake up the human, human spends 90 minutes diagnosing and fixing," you get "AI diagnoses in seconds, proposes a fix, opens a PR, human reviews a diff over coffee."

The gap between "AI suggests a fix" and "AI applies a fix" is just a CI pipeline and a confidence threshold. We already trust automated systems to deploy our code. The only new part is trusting an AI to *write* the patch. And with proper guardrails — test suites, staged rollouts, human approval for high-risk changes — this is entirely tractable.

Here's what this looks like in practice: Your monitoring system detects an error spike. Instead of paging someone, it pulls the relevant logs and traces. It identifies the root cause — say, a null pointer from an API response that changed its schema. It looks at your codebase, finds the affected code path, generates a fix with proper null handling, runs your test suite against it, and opens a pull request. You wake up, review a clean diff, click merge, and move on with your day.

This isn't science fiction. Every individual piece of this pipeline exists today. The question is just who's going to put it together.

## Observability 3.0

Here's how I think about the evolution:

**Observability 1.0** was metrics and dashboards. Nagios, Zabbix, early Grafana. "Here's a graph, good luck." You could see problems, but only if you were looking. This era was defined by the question: *Is something wrong?*

**Observability 2.0** is where most of the industry sits today. Distributed tracing, log correlation, anomaly detection, service maps. Datadog, New Relic, Honeycomb. The defining question became: *What's wrong and why?* A massive improvement. But the answer still terminates in a human's inbox at 3 AM.

**Observability 3.0** is autonomous remediation. The defining question shifts to: *Is it fixed yet?* The system doesn't just find problems — it fixes them. Humans stay in the loop for oversight, not for labor. The on-call engineer becomes a reviewer, not a firefighter.

Most tools in the market are firmly planted in 2.0, competing on marginally better dashboards and slightly smarter alert grouping. They're optimizing a local maximum. The step change — the thing that actually transforms how teams operate — is the leap to 3.0.

## Why Open Source Is Non-Negotiable for This

Here's where this gets interesting from a trust perspective. When your monitoring tool just *watches* your system, you can tolerate it being a black box. You don't love it, but you can live with it. Datadog's agent does mysterious things, but it's only reading data.

When your monitoring tool starts *changing* your system — writing code, opening PRs, modifying configurations, restarting services — the trust calculus changes completely. You need to see exactly what it's doing. You need to audit its decision-making. You need to understand *why* it chose a particular fix and be able to override it.

This is fundamentally incompatible with closed-source SaaS. You can't audit what you can't read. You can't trust what you can't inspect. And when we're talking about a system that autonomously modifies your production infrastructure, "just trust us" isn't going to cut it.

Open source isn't a nice-to-have for autonomous remediation. It's a prerequisite. The code that decides what to fix and how to fix it needs to be as visible and auditable as the code it's fixing. Anything less is a non-starter for any serious engineering team.

## Where This Is Headed

OneUptime is building toward this future. Not because "AI" is a good buzzword for a pitch deck, but because it's the logical next step that nobody's taking. We've been an open-source observability platform — monitoring, incident management, status pages, the full stack. The next layer is closing the loop: not just telling you what broke, but fixing it.

We're not claiming this is solved. It's not. There are hard problems in confidence scoring, blast radius estimation, and knowing when *not* to act. The failure modes of autonomous remediation are different from the failure modes of autonomous detection, and they need to be thought through carefully.

But the direction is clear. The industry spent fifteen years getting really good at waking people up at 3 AM with increasingly precise explanations of what's wrong. The next fifteen should be about letting them sleep.

The tools that figure this out first won't just be better monitoring products. They'll be a fundamentally different category — systems that maintain themselves, with humans providing oversight instead of labor. That's the future worth building toward.

And it has to be open source. Because when your monitoring tool starts fixing your code, you'd better be able to read *its* code too.
