# How to Implement Observability-as-Code with OpenTelemetry Collector Configuration in Git

Author: [nawazdhandala](https://www.github.com/nawazdhandala)

Tags: OpenTelemetry, GitOps, Observability-as-Code, Infrastructure

Description: Manage OpenTelemetry Collector configurations as code in Git with version control, CI validation, and automated deployment pipelines.

Observability infrastructure should be managed the same way you manage application code: in version control, with pull requests, code review, CI validation, and automated deployment. When collector configurations live in a Git repository, every change is auditable, reversible, and reviewable. Teams can propose changes through PRs, and the platform team can enforce standards through automated checks.

This post walks through setting up a complete observability-as-code pipeline for OpenTelemetry Collector configurations.

## Repository Structure

Organize the repository so that each environment and each team has a clear place for their configurations.

```
otel-configs/
  base/                          # Shared base configurations
    receivers.yaml
    exporters.yaml
    processors.yaml
  environments/
    staging/
      collector-config.yaml
    production/
      collector-config.yaml
  teams/
    payments/
      overrides.yaml             # Team-specific processor rules
    search/
      overrides.yaml
  policies/
    naming-conventions.rego       # OPA policies for validation
    cardinality-limits.rego
  scripts/
    validate.sh
    merge-configs.py
  .github/
    workflows/
      validate.yaml
      deploy.yaml
```

## Base Configuration

The base configuration defines shared receivers, exporters, and processors that all environments and teams use.

```yaml
# base/receivers.yaml
# Shared receiver definitions used across all environments.
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
        max_recv_msg_size_mib: 4
      http:
        endpoint: 0.0.0.0:4318
        cors:
          allowed_origins:
            - "https://*.internal.company.com"

  # Host metrics for infrastructure monitoring
  hostmetrics:
    collection_interval: 30s
    scrapers:
      cpu: {}
      memory: {}
      disk: {}
      network: {}
```

```yaml
# base/processors.yaml
# Standard processors applied to all pipelines.
processors:
  # Prevent collector OOM
  memory_limiter:
    check_interval: 5s
    limit_mib: 1024
    spike_limit_mib: 256

  # Batch for efficient export
  batch:
    send_batch_size: 1024
    send_batch_max_size: 2048
    timeout: 10s

  # Add standard resource attributes
  resource/standard:
    attributes:
      - key: platform.managed
        value: true
        action: upsert
      - key: collector.version
        from_attribute: ""
        action: upsert
```

```yaml
# base/exporters.yaml
# Backend exporters - endpoint values are overridden per environment.
exporters:
  otlp/backend:
    endpoint: "${OTEL_BACKEND_ENDPOINT}"
    headers:
      Authorization: "Bearer ${OTEL_AUTH_TOKEN}"
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
```

## Environment-Specific Configuration

Each environment overrides the base with its own settings.

```yaml
# environments/production/collector-config.yaml
# Production collector config - assembled from base + environment overrides.
# This file is generated by the merge script and validated in CI.

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

processors:
  memory_limiter:
    check_interval: 5s
    limit_mib: 2048
    spike_limit_mib: 512

  batch:
    send_batch_size: 2048
    timeout: 5s

  # Production-specific: tail sampling to reduce trace volume
  tail_sampling:
    decision_wait: 10s
    policies:
      # Always keep error traces
      - name: keep-errors
        type: status_code
        status_code:
          status_codes:
            - ERROR
      # Always keep slow traces
      - name: keep-slow
        type: latency
        latency:
          threshold_ms: 1000
      # Sample 10% of everything else
      - name: probabilistic-sample
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

exporters:
  otlp/backend:
    endpoint: "https://otel.oneuptime.com:4317"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, tail_sampling, batch]
      exporters: [otlp/backend]
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlp/backend]
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlp/backend]
```

## Configuration Merge Script

A Python script merges base, environment, and team configurations into a final deployable config.

```python
# scripts/merge_configs.py
# Merges base + environment + team configs into final collector config.
import yaml
import sys
import copy
from pathlib import Path

def deep_merge(base: dict, override: dict) -> dict:
    """Recursively merge override into base."""
    result = copy.deepcopy(base)
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = copy.deepcopy(value)
    return result

def load_yaml(path: Path) -> dict:
    """Load a YAML file, returning empty dict if not found."""
    if not path.exists():
        return {}
    with open(path) as f:
        return yaml.safe_load(f) or {}

def merge_configs(environment: str, team: str = None) -> dict:
    """Build a complete collector config from layers."""
    repo_root = Path(__file__).parent.parent

    # Layer 1: base configs
    base = {}
    for base_file in sorted((repo_root / "base").glob("*.yaml")):
        base = deep_merge(base, load_yaml(base_file))

    # Layer 2: environment overrides
    env_config = load_yaml(repo_root / "environments" / environment / "collector-config.yaml")
    merged = deep_merge(base, env_config)

    # Layer 3: team overrides (if specified)
    if team:
        team_config = load_yaml(repo_root / "teams" / team / "overrides.yaml")
        merged = deep_merge(merged, team_config)

    return merged

if __name__ == "__main__":
    env = sys.argv[1]
    team = sys.argv[2] if len(sys.argv) > 2 else None
    result = merge_configs(env, team)
    print(yaml.dump(result, default_flow_style=False))
```

## CI Validation Pipeline

Every PR that changes a collector config should be validated before merge.

```yaml
# .github/workflows/validate.yaml
name: Validate OTel Configs
on:
  pull_request:
    paths:
      - 'base/**'
      - 'environments/**'
      - 'teams/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          pip install pyyaml
          # Install the OTel Collector for config validation
          wget -q https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.96.0/otelcol-contrib_0.96.0_linux_amd64.tar.gz
          tar xzf otelcol-contrib_0.96.0_linux_amd64.tar.gz

      - name: Merge and validate staging config
        run: |
          python scripts/merge_configs.py staging > /tmp/staging-config.yaml
          ./otelcol-contrib validate --config /tmp/staging-config.yaml

      - name: Merge and validate production config
        run: |
          python scripts/merge_configs.py production > /tmp/production-config.yaml
          ./otelcol-contrib validate --config /tmp/production-config.yaml

      - name: Check naming conventions
        run: |
          python scripts/check_naming.py /tmp/production-config.yaml

      - name: Diff against current deployed config
        run: |
          # Show what will change when this PR is deployed
          python scripts/merge_configs.py production > /tmp/new-config.yaml
          diff -u deployed/production-config.yaml /tmp/new-config.yaml || true
```

## Automated Deployment

After a PR is merged, deploy the new configuration automatically.

```yaml
# .github/workflows/deploy.yaml
name: Deploy OTel Configs
on:
  push:
    branches: [main]
    paths:
      - 'base/**'
      - 'environments/**'
      - 'teams/**'

jobs:
  deploy-staging:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Generate staging config
        run: python scripts/merge_configs.py staging > /tmp/staging-config.yaml

      - name: Deploy to staging
        run: |
          # Update the ConfigMap in the staging cluster
          kubectl --context staging create configmap otel-collector-config \
            --from-file=config.yaml=/tmp/staging-config.yaml \
            --dry-run=client -o yaml | kubectl apply -f -
          # Restart the collector to pick up the new config
          kubectl --context staging rollout restart deployment/otel-collector

  deploy-production:
    needs: deploy-staging
    runs-on: ubuntu-latest
    # Wait for staging validation before deploying to prod
    environment: production
    steps:
      - uses: actions/checkout@v4

      - name: Generate production config
        run: python scripts/merge_configs.py production > /tmp/production-config.yaml

      - name: Deploy to production
        run: |
          kubectl --context production create configmap otel-collector-config \
            --from-file=config.yaml=/tmp/production-config.yaml \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl --context production rollout restart deployment/otel-collector
```

## Policy Enforcement with OPA

Use Open Policy Agent to enforce organizational rules on configurations.

```rego
# policies/naming-conventions.rego
# Enforce that all custom processors follow naming conventions.
package otel.naming

deny[msg] {
    some processor_name
    input.processors[processor_name]
    # Custom processors must use team prefix
    not startswith(processor_name, "filter/")
    not startswith(processor_name, "attributes/")
    not startswith(processor_name, "resource/")
    # Standard processors are exempt
    not processor_name in {"batch", "memory_limiter", "tail_sampling"}
    msg := sprintf("Processor '%s' does not follow naming convention", [processor_name])
}

deny[msg] {
    some pipeline_name
    input.service.pipelines[pipeline_name]
    pipeline := input.service.pipelines[pipeline_name]
    # Every pipeline must include memory_limiter
    not "memory_limiter" in pipeline.processors
    msg := sprintf("Pipeline '%s' is missing the memory_limiter processor", [pipeline_name])
}
```

## Rollback Strategy

Because configurations are in Git, rollbacks are straightforward.

```bash
# Find the last known good commit
git log --oneline environments/production/

# Revert to the previous config
git revert HEAD

# Or for an emergency, deploy a specific commit directly
git checkout abc123 -- environments/production/collector-config.yaml
python scripts/merge_configs.py production > /tmp/rollback-config.yaml
kubectl create configmap otel-collector-config \
  --from-file=config.yaml=/tmp/rollback-config.yaml \
  --dry-run=client -o yaml | kubectl apply -f -
kubectl rollout restart deployment/otel-collector
```

## Why This Approach Works

Treating collector configurations as code gives you the same safety net you rely on for application deployments: peer review catches mistakes, CI prevents invalid configs from reaching production, and Git history provides a complete audit trail. When an observability issue occurs, you can trace it back to the exact PR that changed the configuration. This is significantly better than SSH-ing into a collector and editing a config file by hand.
