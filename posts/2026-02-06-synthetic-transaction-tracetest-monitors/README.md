# How to Build Synthetic Transaction Tests with Tracetest Monitors and OpenTelemetry

Author: [nawazdhandala](https://www.github.com/nawazdhandala)

Tags: OpenTelemetry, Tracetest, Synthetic Monitoring, Transaction Tests, Observability

Description: Use Tracetest monitors to run continuous synthetic transaction tests that validate both API responses and backend trace behavior.

Synthetic monitoring sends fake requests to your production system on a schedule and alerts you when something breaks. Traditional synthetic monitors only check HTTP status codes and response times. Tracetest monitors go deeper: they validate the entire distributed trace generated by each synthetic request. If the API returns 200 but the database write was skipped, a Tracetest monitor will catch it.

## What Are Tracetest Monitors

Tracetest monitors are scheduled test runs that execute a defined test at regular intervals. Each run triggers an HTTP request (or gRPC call), waits for the resulting trace to appear in your trace backend, and then runs assertions against individual spans within that trace.

## Setting Up Tracetest for Monitoring

Start by deploying Tracetest alongside your production trace backend:

```yaml
# tracetest-config.yaml
type: DataStore
spec:
  name: production-traces
  type: otlp
  otlp:
    endpoint: otel-collector:4317
    tls:
      insecure: true
```

Apply the configuration:

```bash
tracetest apply datastore -f tracetest-config.yaml
```

## Creating a Synthetic Transaction Test

Define a test that exercises a critical user flow. This example tests an order placement:

```yaml
# tests/order-placement.yaml
type: Test
spec:
  id: synthetic-order-placement
  name: "Synthetic: Order Placement"
  description: "Validates the complete order placement flow via trace assertions"
  trigger:
    type: http
    httpRequest:
      method: POST
      url: https://api.example.com/api/orders
      headers:
        - key: Content-Type
          value: application/json
        - key: X-Synthetic-Test
          value: "true"
        - key: Authorization
          value: "Bearer ${env:SYNTHETIC_TEST_TOKEN}"
      body: |
        {
          "user_id": "synthetic-user",
          "items": [{"sku": "TEST-ITEM", "qty": 1}],
          "synthetic": true
        }
  specs:
    # Check the trigger response
    - selector: span[tracetest.span.type="general" name="Tracetest trigger"]
      assertions:
        - attr:tracetest.response.status = 201

    # Verify order service processed the request
    - selector: span[name="POST /api/orders" service.name="order-service"]
      assertions:
        - attr:http.response.status_code = 201
        - attr:tracetest.span.duration < 500ms

    # Verify database write
    - selector: span[name="INSERT INTO orders" service.name="order-service"]
      assertions:
        - attr:db.system = "postgresql"
        - attr:tracetest.span.duration < 100ms

    # Verify inventory was updated
    - selector: span[name="update_inventory" service.name="inventory-service"]
      assertions:
        - attr:inventory.updated = true

    # Verify no error spans in the entire trace
    - selector: span[status.code="ERROR"]
      assertions:
        - attr:tracetest.selected_spans.count = 0

  outputs:
    - name: ORDER_ID
      selector: span[name="POST /api/orders"]
      value: attr:order.id
```

Apply the test:

```bash
tracetest apply test -f tests/order-placement.yaml
```

## Setting Up the Monitor Schedule

Create a monitor that runs this test every 5 minutes:

```yaml
# monitors/order-placement-monitor.yaml
type: Monitor
spec:
  id: order-placement-monitor
  name: "Monitor: Order Placement (every 5 min)"
  enabled: true
  tests:
    - synthetic-order-placement
  schedule:
    cron: "*/5 * * * *"
  alerts:
    - type: webhook
      webhook:
        url: https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
        headers:
          - key: Content-Type
            value: application/json
        body: |
          {
            "text": "Synthetic order placement test failed. Check Tracetest for details: ${TRACETEST_URL}/test/synthetic-order-placement/run/${RUN_ID}"
          }
```

Apply the monitor:

```bash
tracetest apply monitor -f monitors/order-placement-monitor.yaml
```

## Building Multi-Step Transaction Tests

Real user flows involve multiple API calls. Chain them together using test outputs:

```yaml
# tests/full-checkout-flow.yaml
type: Test
spec:
  id: synthetic-full-checkout
  name: "Synthetic: Full Checkout Flow"
  trigger:
    type: http
    httpRequest:
      method: POST
      url: https://api.example.com/api/cart
      headers:
        - key: Content-Type
          value: application/json
        - key: Authorization
          value: "Bearer ${env:SYNTHETIC_TEST_TOKEN}"
      body: |
        {
          "user_id": "synthetic-user",
          "item_sku": "TEST-ITEM",
          "quantity": 1
        }
  specs:
    - selector: span[name="POST /api/cart" service.name="cart-service"]
      assertions:
        - attr:http.response.status_code = 200
        - attr:cart.items_count >= 1

    - selector: span[name="HSET cart:*" service.name="cart-service"]
      assertions:
        - attr:db.system = "redis"
        - attr:tracetest.span.duration < 50ms
  outputs:
    - name: CART_ID
      selector: span[name="POST /api/cart"]
      value: attr:cart.id
```

## Filtering Synthetic Traffic

Make sure synthetic test traffic does not pollute your production metrics. Tag synthetic requests and filter them in your collector:

```yaml
# otel-collector-config.yaml
processors:
  filter/synthetic:
    traces:
      span:
        - 'attributes["http.request.header.x_synthetic_test"] == "true"'

  attributes/tag-synthetic:
    actions:
      - key: synthetic_test
        value: true
        action: upsert

service:
  pipelines:
    # Production traces pipeline (excludes synthetic)
    traces/production:
      receivers: [otlp]
      processors: [filter/synthetic, batch]
      exporters: [otlp/production-backend]

    # Synthetic traces pipeline (for Tracetest)
    traces/synthetic:
      receivers: [otlp]
      processors: [attributes/tag-synthetic, batch]
      exporters: [otlp/tracetest-backend]
```

## Cleaning Up Synthetic Data

Synthetic tests create real data in your system. Clean it up automatically:

```python
# cleanup_synthetic.py
import requests
import schedule
import time

API_URL = "https://api.example.com"
CLEANUP_TOKEN = os.environ["CLEANUP_TOKEN"]

def cleanup_synthetic_orders():
    """Delete orders created by synthetic tests."""
    headers = {
        "Authorization": f"Bearer {CLEANUP_TOKEN}",
        "Content-Type": "application/json",
    }

    # Find synthetic orders
    resp = requests.get(
        f"{API_URL}/api/orders",
        params={"user_id": "synthetic-user", "limit": 100},
        headers=headers,
    )

    orders = resp.json().get("orders", [])
    deleted = 0
    for order in orders:
        if order.get("synthetic", False):
            requests.delete(
                f"{API_URL}/api/orders/{order['id']}",
                headers=headers,
            )
            deleted += 1

    print(f"Cleaned up {deleted} synthetic orders")

# Run cleanup every hour
schedule.every(1).hours.do(cleanup_synthetic_orders)

while True:
    schedule.run_pending()
    time.sleep(60)
```

## Monitoring the Monitor

Check monitor health via the Tracetest API:

```bash
# Get recent monitor runs
tracetest list runs --monitor order-placement-monitor --limit 20

# Check failure rate
tracetest get monitor order-placement-monitor --output json \
  | jq '.runs | map(select(.state == "FAILED")) | length'
```

Synthetic transaction tests with Tracetest give you continuous validation that your system works end to end. Unlike traditional uptime checks that just ping an endpoint, these tests verify the entire internal behavior of your distributed system. When the monitor catches a failure, you already have the trace showing exactly what went wrong.
