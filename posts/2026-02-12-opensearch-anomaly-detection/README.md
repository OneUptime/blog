# How to Use OpenSearch Anomaly Detection

Author: [nawazdhandala](https://github.com/nawazdhandala)

Tags: AWS, OpenSearch, Anomaly Detection, Monitoring

Description: Learn how to configure and use the anomaly detection feature in Amazon OpenSearch to automatically identify unusual patterns in your time-series data.

---

Staring at dashboards waiting for something to go wrong isn't a scalable strategy. OpenSearch's built-in anomaly detection uses machine learning to automatically spot unusual patterns in your time-series data - spikes in error rates, drops in throughput, unusual latency patterns, and more. It works on any numeric data in your indexes without requiring you to define specific thresholds.

The anomaly detection plugin uses the Random Cut Forest (RCF) algorithm, which is good at detecting anomalies in streaming data with seasonal patterns. You tell it what to watch, and it learns what "normal" looks like over time.

## How It Works

The anomaly detection workflow has three main pieces:

1. **Detector** - Defines what data to monitor and how to aggregate it
2. **Feature** - A specific metric to track within a detector (like average response time or count of errors)
3. **Results** - Anomaly scores and confidence levels generated by the ML model

The detector runs on a schedule, aggregates data for each interval, feeds it to the RCF model, and produces an anomaly score. When the score exceeds a threshold (called the anomaly grade), it's flagged as an anomaly.

## Creating a Detector via the API

Here's how to create a detector that monitors error rates across your application services:

```bash
# Create an anomaly detector for error rate monitoring
curl -XPOST "https://search-domain.us-east-1.es.amazonaws.com/_plugins/_anomaly_detection/detectors" \
    -H "Content-Type: application/json" \
    -d '{
    "name": "error-rate-detector",
    "description": "Detects anomalous error rates across services",
    "time_field": "timestamp",
    "indices": ["logs-*"],
    "filter_query": {
        "bool": {
            "filter": [
                {"term": {"level": "ERROR"}}
            ]
        }
    },
    "detection_interval": {
        "period": {
            "interval": 5,
            "unit": "Minutes"
        }
    },
    "window_delay": {
        "period": {
            "interval": 1,
            "unit": "Minutes"
        }
    },
    "feature_attributes": [
        {
            "feature_name": "error_count",
            "feature_enabled": true,
            "aggregation_query": {
                "error_count": {
                    "value_count": {
                        "field": "level"
                    }
                }
            }
        }
    ],
    "category_field": ["service"]
}'
```

Let's break down the important parts:

- **detection_interval** - How often the detector runs (5 minutes here)
- **window_delay** - Wait period to account for data ingestion lag (1 minute)
- **feature_attributes** - The metrics to track (error count in this case)
- **category_field** - Split the detection by service, so each service gets its own baseline

The `category_field` is really powerful. Instead of one detector that averages errors across all services, you get per-service anomaly detection. A service that normally produces 100 errors/minute won't mask a service that goes from 0 to 50 errors/minute.

## Multi-Feature Detectors

You can monitor multiple metrics in a single detector. Here's one that tracks both error count and response time:

```bash
# Detector with multiple features for comprehensive monitoring
curl -XPOST "https://search-domain.us-east-1.es.amazonaws.com/_plugins/_anomaly_detection/detectors" \
    -H "Content-Type: application/json" \
    -d '{
    "name": "service-health-detector",
    "description": "Monitors error rates and response times per service",
    "time_field": "timestamp",
    "indices": ["app-logs-*"],
    "detection_interval": {
        "period": {"interval": 5, "unit": "Minutes"}
    },
    "window_delay": {
        "period": {"interval": 2, "unit": "Minutes"}
    },
    "feature_attributes": [
        {
            "feature_name": "error_count",
            "feature_enabled": true,
            "aggregation_query": {
                "error_count": {
                    "filter": {
                        "term": {"level": "ERROR"}
                    },
                    "aggs": {
                        "count": {
                            "value_count": {"field": "level"}
                        }
                    }
                }
            }
        },
        {
            "feature_name": "avg_response_time",
            "feature_enabled": true,
            "aggregation_query": {
                "avg_response_time": {
                    "avg": {
                        "field": "response_time_ms"
                    }
                }
            }
        },
        {
            "feature_name": "p99_response_time",
            "feature_enabled": true,
            "aggregation_query": {
                "p99_response_time": {
                    "percentiles": {
                        "field": "response_time_ms",
                        "percents": [99]
                    }
                }
            }
        }
    ],
    "category_field": ["service"]
}'
```

## Starting the Detector

After creating a detector, you need to start it:

```bash
# Start the anomaly detector
curl -XPOST "https://search-domain.us-east-1.es.amazonaws.com/_plugins/_anomaly_detection/detectors/<detector-id>/_start"
```

It takes some time for the model to learn what's normal. The RCF algorithm needs at least 400 data points (about 33 hours with 5-minute intervals) to build a reliable baseline. During the warm-up period, you'll see anomaly grades of 0 even if the data looks unusual.

## Querying Anomaly Results

Once the detector is running, you can query its results:

```bash
# Get anomaly results for a specific detector
# Shows recent anomalies with their scores and confidence
curl -XPOST "https://search-domain.us-east-1.es.amazonaws.com/_plugins/_anomaly_detection/detectors/<detector-id>/results/_search" \
    -H "Content-Type: application/json" \
    -d '{
    "size": 20,
    "query": {
        "bool": {
            "filter": [
                {
                    "range": {
                        "anomaly_grade": {
                            "gt": 0
                        }
                    }
                },
                {
                    "range": {
                        "data_start_time": {
                            "gte": "now-24h"
                        }
                    }
                }
            ]
        }
    },
    "sort": [
        {"anomaly_grade": {"order": "desc"}}
    ]
}'
```

The response includes:
- **anomaly_grade** - Severity from 0 to 1 (higher means more anomalous)
- **confidence** - How confident the model is (increases as it sees more data)
- **feature_data** - The actual metric values that triggered the anomaly
- **entity** - Which category (service) had the anomaly

## Historical Analysis

If you want to check past data for anomalies, you can run a historical detection:

```bash
# Run historical anomaly detection on past data
curl -XPOST "https://search-domain.us-east-1.es.amazonaws.com/_plugins/_anomaly_detection/detectors/<detector-id>/_start" \
    -H "Content-Type: application/json" \
    -d '{
    "start_time": 1706745600000,
    "end_time": 1707350400000
}'
```

Historical detection is useful for investigating past incidents. You can point it at last week's data and see if the ML model would have caught an issue your team found manually.

## Connecting Anomalies to Alerts

Detecting anomalies is only useful if someone gets notified. You can connect your anomaly detector to OpenSearch alerting:

```bash
# Create an alert monitor that triggers on anomaly detection results
curl -XPOST "https://search-domain.us-east-1.es.amazonaws.com/_plugins/_alerting/monitors" \
    -H "Content-Type: application/json" \
    -d '{
    "type": "monitor",
    "name": "High Anomaly Score Alert",
    "monitor_type": "query_level_monitor",
    "enabled": true,
    "schedule": {
        "period": {
            "interval": 5,
            "unit": "MINUTES"
        }
    },
    "inputs": [
        {
            "search": {
                "indices": [".opendistro-anomaly-results-*"],
                "query": {
                    "size": 0,
                    "query": {
                        "bool": {
                            "filter": [
                                {"term": {"detector_id": "<detector-id>"}},
                                {"range": {"anomaly_grade": {"gte": 0.7}}},
                                {"range": {"data_end_time": {"gte": "{{period_end}}||-5m"}}}
                            ]
                        }
                    },
                    "aggs": {
                        "anomaly_count": {
                            "value_count": {"field": "anomaly_grade"}
                        }
                    }
                }
            }
        }
    ],
    "triggers": [
        {
            "name": "High anomaly trigger",
            "severity": "1",
            "condition": {
                "script": {
                    "source": "ctx.results[0].aggregations.anomaly_count.value > 0",
                    "lang": "painless"
                }
            },
            "actions": [
                {
                    "name": "Notify team",
                    "destination_id": "<notification-destination-id>",
                    "message_template": {
                        "source": "Anomaly detected! Grade: {{ctx.results[0].hits.total.value}} anomalous intervals in the last 5 minutes."
                    }
                }
            ]
        }
    ]
}'
```

## Tuning Tips

**Set appropriate detection intervals.** Five minutes works well for most application metrics. One minute is too noisy for most workloads, and 15 minutes might miss short-lived issues.

**Use category fields wisely.** Each unique category value creates a separate model. If your `service` field has 500 unique values, that's 500 models running concurrently. Keep it under 100 categories per detector for performance.

**Filter out noise.** If your logs include health checks or synthetic traffic that you don't want to detect anomalies on, add a filter query to exclude them from the detector.

**Give the model time.** Resist the urge to tweak the detector in the first 48 hours. The model needs time to learn seasonal patterns like weekday vs. weekend traffic differences.

For a broader monitoring strategy that includes anomaly detection, check out [building log analytics dashboards](https://oneuptime.com/blog/post/2026-02-12-log-analytics-dashboards-opensearch/view) and [setting up OpenSearch alerting](https://oneuptime.com/blog/post/2026-02-12-opensearch-alerting/view).
