# How to Create Log-Based Metrics in Cloud Logging for Custom Monitoring

Author: [nawazdhandala](https://www.github.com/nawazdhandala)

Tags: GCP, Cloud Logging, Log-Based Metrics, Cloud Monitoring, Custom Metrics

Description: Learn how to create log-based metrics in Cloud Logging to turn log data into Cloud Monitoring metrics for dashboards and alerting policies.

---

Not everything you want to monitor shows up as a built-in metric. Maybe you want to track how often a specific error message appears in your application logs, or count the number of failed login attempts, or measure the distribution of response times logged by your application. Log-based metrics let you extract this kind of data from your logs and turn it into Cloud Monitoring metrics that you can chart, alert on, and analyze just like any other metric.

In this post, I will explain the two types of log-based metrics and walk through creating them for real-world use cases.

## What Are Log-Based Metrics?

Log-based metrics are Cloud Monitoring metrics that are derived from log entries. Every time a log entry matches a filter you define, the metric is updated. There are two types:

### Counter Metrics

Counter metrics count the number of log entries that match a filter. For example, "how many ERROR-level log entries were generated in the last hour?"

### Distribution Metrics

Distribution metrics track the statistical distribution of values extracted from log entries. For example, "what is the latency distribution of API requests as logged by my application?"

Cloud Logging also has several system-defined log-based metrics that are created automatically, like `logging.googleapis.com/log_entry_count` and `logging.googleapis.com/byte_count`.

## Creating Counter Metrics

### Counting Error Logs

The simplest and most common use case - count how many errors your application produces:

```bash
# Create a counter metric for application error logs
gcloud logging metrics create application-error-count \
  --description="Count of application error log entries" \
  --log-filter='resource.type="cloud_run_revision" AND severity>=ERROR'
```

This creates a metric named `logging.googleapis.com/user/application-error-count` that increments every time an error log is generated by a Cloud Run service.

### Counting Specific Error Patterns

You can be more specific with the filter to count particular types of errors:

```bash
# Count database connection errors
gcloud logging metrics create db-connection-errors \
  --description="Count of database connection error log entries" \
  --log-filter='textPayload=~"connection refused" OR textPayload=~"connection timed out" OR jsonPayload.message=~"database connection failed"'
```

### Counting with Labels

Labels let you break down a metric by different dimensions. For example, counting errors per service:

```bash
# Create a counter metric with labels for per-service breakdown
gcloud logging metrics create service-error-count \
  --description="Error count by service name" \
  --log-filter='severity>=ERROR AND resource.type="cloud_run_revision"' \
  --label-extractors='service_name=EXTRACT(resource.labels.service_name)'
```

Now you can group this metric by `service_name` in your dashboards and alerts.

## Creating Distribution Metrics

Distribution metrics are more complex but far more useful for performance monitoring. They extract a numeric value from each matching log entry and build a histogram distribution.

### Tracking Application Response Times

If your application logs response times, you can turn them into a distribution metric:

```bash
# Create a distribution metric for response times from JSON logs
gcloud logging metrics create api-response-time \
  --description="Distribution of API response times from application logs" \
  --log-filter='jsonPayload.type="api_request" AND jsonPayload.response_time_ms>0' \
  --value-extractor='EXTRACT(jsonPayload.response_time_ms)' \
  --bucket-options='explicit-buckets=boundaries:10,25,50,100,250,500,1000,2500,5000,10000'
```

The `--bucket-options` defines the histogram buckets. Requests faster than 10ms go in the first bucket, 10-25ms in the second, and so on.

### Tracking Request Payload Sizes

```bash
# Distribution metric for HTTP request payload sizes
gcloud logging metrics create request-payload-size \
  --description="Distribution of HTTP request body sizes" \
  --log-filter='httpRequest.requestSize>0' \
  --value-extractor='EXTRACT(httpRequest.requestSize)' \
  --bucket-options='exponential-buckets=num-finite-buckets:20,growth-factor:2,scale:100'
```

Exponential buckets work well for values that span several orders of magnitude.

## Creating Log-Based Metrics via the Console

For a more visual approach:

1. Go to **Logging** > **Log-based Metrics**
2. Click **Create Metric**
3. Choose the metric type (Counter or Distribution)
4. Enter the filter query
5. For distribution metrics, specify the value extractor and bucket configuration
6. Optionally add labels
7. Click **Create Metric**

## Using Log-Based Metrics in Dashboards

Once created, log-based metrics appear in Cloud Monitoring like any other metric. You can add them to dashboards:

```
# Query a user-defined log-based metric in MQL
fetch global
| metric 'logging.googleapis.com/user/application-error-count'
| group_by [], [val: sum(value.application_error_count)]
| every 5m
```

For distribution metrics, you can query specific percentiles:

```
# P95 of the response time distribution metric
fetch global
| metric 'logging.googleapis.com/user/api-response-time'
| group_by [], [val: percentile(value.api_response_time, 95)]
| every 5m
```

## Creating Alerts on Log-Based Metrics

Log-based metrics integrate with Cloud Monitoring alerting. Here is an alerting policy that fires when the error count exceeds a threshold:

```json
{
  "displayName": "High Application Error Rate",
  "combiner": "OR",
  "conditions": [
    {
      "displayName": "Error count above 100 per minute",
      "conditionThreshold": {
        "filter": "metric.type=\"logging.googleapis.com/user/application-error-count\"",
        "comparison": "COMPARISON_GT",
        "thresholdValue": 100,
        "duration": "60s",
        "aggregations": [
          {
            "alignmentPeriod": "60s",
            "perSeriesAligner": "ALIGN_RATE"
          }
        ]
      }
    }
  ]
}
```

For distribution metrics, you can alert on percentiles:

```json
{
  "displayName": "High API Response Time",
  "combiner": "OR",
  "conditions": [
    {
      "displayName": "P95 response time above 2 seconds",
      "conditionThreshold": {
        "filter": "metric.type=\"logging.googleapis.com/user/api-response-time\"",
        "comparison": "COMPARISON_GT",
        "thresholdValue": 2000,
        "duration": "300s",
        "aggregations": [
          {
            "alignmentPeriod": "60s",
            "perSeriesAligner": "ALIGN_PERCENTILE_95"
          }
        ]
      }
    }
  ]
}
```

## Terraform Configuration

```hcl
# Counter log-based metric
resource "google_logging_metric" "error_count" {
  name        = "application-error-count"
  description = "Count of application error log entries"
  filter      = "resource.type=\"cloud_run_revision\" AND severity>=ERROR"

  metric_descriptor {
    metric_kind = "DELTA"
    value_type  = "INT64"
  }
}

# Distribution log-based metric with labels
resource "google_logging_metric" "response_time" {
  name        = "api-response-time"
  description = "Distribution of API response times"
  filter      = "jsonPayload.type=\"api_request\""

  metric_descriptor {
    metric_kind = "DELTA"
    value_type  = "DISTRIBUTION"

    labels {
      key         = "service_name"
      value_type  = "STRING"
      description = "Name of the service"
    }
  }

  value_extractor = "EXTRACT(jsonPayload.response_time_ms)"

  label_extractors = {
    "service_name" = "EXTRACT(resource.labels.service_name)"
  }

  bucket_options {
    explicit_buckets {
      bounds = [10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000]
    }
  }
}

# Alert on the error count metric
resource "google_monitoring_alert_policy" "error_rate" {
  display_name = "High Error Rate from Log-Based Metric"
  combiner     = "OR"

  conditions {
    display_name = "Error count above threshold"

    condition_threshold {
      filter          = "metric.type=\"logging.googleapis.com/user/${google_logging_metric.error_count.name}\""
      comparison      = "COMPARISON_GT"
      threshold_value = 100
      duration        = "60s"

      aggregations {
        alignment_period   = "60s"
        per_series_aligner = "ALIGN_RATE"
      }
    }
  }

  notification_channels = var.notification_channels
}
```

## Best Practices

1. **Keep filters specific**: Broad filters create noisy metrics. The more specific your filter, the more useful the metric.

2. **Use labels wisely**: Labels increase the cardinality of your metric. Each unique combination of label values creates a separate time series. Too many labels can make the metric expensive and hard to query.

3. **Choose appropriate bucket boundaries**: For distribution metrics, set bucket boundaries that match your application's expected value range. If most response times are 50-200ms, do not start your first bucket at 1000ms.

4. **Monitor the metric itself**: After creating a log-based metric, check that it is actually capturing data. Go to Metrics Explorer and query the metric to verify.

5. **Consider log volume**: Log-based metrics are evaluated on every matching log entry. If your filter matches millions of entries per minute, make sure the metric configuration is efficient.

## Wrapping Up

Log-based metrics bridge the gap between unstructured log data and structured monitoring. Instead of manually searching through logs to understand error trends or performance patterns, you define a metric once and it continuously extracts the data you care about. Counter metrics tell you "how often" and distribution metrics tell you "how much." Together, they turn your logs into actionable monitoring signals that integrate seamlessly with Cloud Monitoring dashboards and alerts.
